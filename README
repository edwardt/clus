
Cluster is a simple tool for installing, configuring, and
bootstrapping a cluster of nodes - primarily Hibari nodes.  The
cluster tool requires one installer node and one or more target nodes
for the Hibari cluster.  The installer node can be a different node or
can be one of the target nodes.  The cluster tool requires an
installing user (that is you) and it must be different than the user
for Hibari on the target nodes.

NOTE: This tool probably won't meet the needs of all users.
Nevertheless, this tool should meet the needs of most users.

CAUTION: This tool's "target node" recipe is currently Linux-centric
(e.g. useradd, userdel, ...).  Patches and contributions for other OS
and platforms are welcome.

Your Hibari cluster nodes must have following tools/environments
ready.  For further information and help for related tools, please
refer to the following links:

- Git - http://git-scm.com/
  * *Git 1.5.4 or newer*
  * _required for Repo and GitHub_
- GitHub - https://github.com
- Erlang - http://www.erlang.org/
  * *R13B04 or newer*
  * *R14B01 has been tested most recently*
- Python - http://www.python.org
  * *Python 2.4 or newer (CAUTION: Python 3.x might be too new)*
  * _required for Repo_
- Repo - http://source.android.com/source/git-repo.html
- SSH - http://inside.mines.edu/~gmurray/HowTo/sshNotes.html
- Expect - http://www.nist.gov/el/msid/expect.cfm
  * _required only for installer node_

So far, there are no "known" version requirements for Bash, Perl,
Expect, and SSH.


To Setup a Hibari Cluster
=========================

Assumptions:

A. 1 installer node
  * Bash, Git, Perl, Python, Expect, and SSH (client) is installed on
    installer node
  * Your login account ($USER) exists on installer node with ssh
    private/public keys and ssh agent setup to enable password-less
    ssh login
  * /etc/hosts file on installer node contains entries for all target
    nodes

Example /etc/hosts file entries for Network A host addresses:
------
10.181.165.230  dev1.your-domain.com    dev1
10.181.165.231  dev2.your-domain.com    dev2
10.181.165.232  dev3.your-domain.com    dev3
------

B. 1 or more cluster target nodes (e.g. dev1, dev2, dev3)
  * Your login account ($USER) exists on target nodes
  * Your login account ($USER) is enabled with password-less sudo
    access on target nodes
  * Your login account ($USER) is accessible with password-less ssh
    login on target nodes

  * Erlang and SSH (server) is installed on target nodes
  * `hostname -s` equals nodeX for each target node respectively and
    /etc/hosts file on target nodes contains entries for all target
    nodes
  * Network A and Network B is setup and active (see note below)
  * The base home directory for the node user exists
    (e.g. "/usr/local/var/lib")

    NOTE: The base home directory can be changed by environment
    variable.  See src/lib/clus/priv/clus.sh for details.

C. Cluster configuration file (e.g. hibari.config)
  * Erlang base directory
  * Hibari Admin nodes
  * Hibari Brick nodes
  * Hibari Bricks per Chain value (i.e. replication factor)
  * All Hibari nodes (union of Admin and Brick nodes)
  * All Hibari nodes Network A and Network B ip addresses plus Network
    broadcast addresses and Network A tiebreaker address
  * All Heartbeat UDP ports

Example configuration file (e.g. hibari.config) for a three node
cluster that uses the same physical network for Network A and Network
B.  Please prepare a similar file on your installer node:

------
ERLANG_DIR=/usr/local/lib/erlang

ADMIN_NODES=(dev1 dev2 dev3)
BRICK_NODES=(dev1 dev2 dev3)
BRICKS_PER_CHAIN=2

ALL_NODES=(dev1 dev2 dev3)
ALL_NETA_ADDRS=("10.181.165.230" "10.181.165.231" "10.181.165.232")
ALL_NETB_ADDRS=("10.181.165.230" "10.181.165.231" "10.181.165.232")
ALL_NETA_BCAST="10.181.165.255"
ALL_NETB_BCAST="10.181.165.255"
ALL_NETA_TIEBREAKER="10.181.165.1"

ALL_HEART_UDP_PORT="63099"
ALL_HEART_XMIT_UDP_PORT="63100"
------

    NOTE: See here
    http://hibari.github.com/hibari-doc/hibari-sysadmin-guide.en.html#partition-detector
    for further information regarding the network partition detector
    application.  Additional information for the application's
    configuration is embedded in the partition-detector's OTP
    application source file
    (https://github.com/hibari/partition-detector/raw/master/src/partition_detector.app.src).

    NOTE: In a production setting, Network A and Network B should be
    physically different networks and network interfaces.  However,
    the same network can be used (as in this example) for Network A
    and Network B for testing and development purposes.


Instructions:

Example how to prepare installing user
======================================

1. Setup your user (i.e. your login - $USER) on all Hibari nodes if
   not already existing.  This user will be used for Hibari
   installation purposes.

    a. As root user, add your user to all of the Hibari nodes and
       grant sudo access for your user.

       ------
       $ useradd $USER
       $ passwd $USER
       $ vi /etc/sudoers
         # append the following line and save it
         $USER  ALL=(ALL)       NOPASSWD: ALL
       ------

       NOTE: If you get "sudo: sorry, you must have a tty to run sudo"
       error while testing 'sudo', consider to comment out following
       line inside of the /etc/sudoers file:

       ------
       $ vi /etc/sudoers
         Defaults    requiretty
       ------

    b. Create a new ssh private/public key for your user on the first
       node (or one of the nodes of your choosing) of all the Hibari
       nodes.

       ------
       $ ssh-keygen
         # enter your password for the private key
       $ eval `ssh-agent`
       $ ssh-add ~/.ssh/id_rsa
         # re-enter your password for the private key
       ------

    c. Append an entry for the first node to your ~/.ssh/known_hosts
       file on each of the Hibari nodes and append an entry to your
       ~/.ssh/authorized_keys file on all of the Hibari nodes for your
       public ssh key (on the first node).

       ------
       $ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@devN
       $ ssh $USER@devN hostname
       ------

       NOTE: if your installer node will be the one of Hibari cluster
       nodes, make sure ssh-copy-id to installer node also.


Example how to prepare installer node
=====================================

1. Configure your e-mail and name for Git

    $ git config --global user.email "you@example.com"
    $ git config --global user.name "Your Name"

2. Get and install Repo

    $ mkdir -p ~/bin
    $ curl http://android.git.kernel.org/repo > ~/bin/repo
    $ chmod a+x ~/bin/repo

3. Create working directory

    $ mkdir working-directory-name

4. Initialize clus working directory

    $ mkdir working-directory-name/clus
    $ cd working-directory-name/clus
    $ ~/bin/repo init -u git://github.com/norton/manifests.git -m clus-default.xml

5. Download Git repositories

    $ cd working-directory-name/clus
    $ ~/bin/repo sync


Example how to create All Hibari nodes
======================================

1. Create (or re-create) hibari user on All Hibari nodes

   NOTE: If your ssh private key is protected by a password, please
   make sure your private key is registered with the ssh agent before
   proceeding.

   $ cd working-directory-name
   $ for i in dev1 dev2 dev3 ; do ./clus/src/lib/clus/priv/clus.sh -f init hibari $i ; done
hibari@dev1
hibari@dev2
hibari@dev3

   CAUTION: The -f option will forcefully delete and then re-create
   the hibari user on the target node.


2. Download to installer node. Next, Build and Setup hibari package on
   All Hibari nodes
   $ cd working-directory-name
   $ ./clus/src/lib/clus/priv/clus-hibari.sh -f init hibari hibari.config
hibari@dev1
hibari@dev2
hibari@dev3


3. Start hibari package on All Hibari nodes
   $ cd working-directory-name
   $ ./clus/src/lib/clus/priv/clus-hibari.sh -f start hibari hibari.config
hibari@dev1
hibari@dev2
hibari@dev3


4. Bootstrap hibari package on first Hibari Admin node
   $ cd working-directory-name
   $ ./clus/src/lib/clus/priv/clus-hibari.sh -f bootstrap hibari hibari.config
hibari@dev1 => hibari@dev1 hibari@dev2 hibari@dev3

   NOTE: If bootstrapping fails due to "another_admin_server_running"
   error, please stop the other Hibari cluster(s) running on the
   network or repeat this process from step 1. with udp ports
   (i.e. ALL_HEART_UDP_PORT and ALL_HEART_XMIT_UDP_PORT) that are not
   in use by other applications or another Hibari cluster.


5. Open "Hibari Web Administration" Page
   $ your-favorite-browser http://dev1:23080


6. Stop hibari package on All Hibari nodes
   $ cd working-directory-name
   $ ./clus/src/lib/clus/priv/clus-hibari.sh -f stop hibari hibari.config
ok
ok
ok
hibari@dev1
hibari@dev2
hibari@dev3
